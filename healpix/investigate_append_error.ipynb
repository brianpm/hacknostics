{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.3 | packaged by conda-forge | (main, Apr 15 2024, 18:38:13) [GCC 12.3.0]\n",
      "Numpy 2.1.3\n",
      "Xarray 2025.1.2\n",
      "Dask 2025.2.0\n",
      "Healpix 2024.2\n",
      "Zarr 3.0.5\n",
      "EasyGems doesn't provide a version attribute.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import healpix as hp\n",
    "\n",
    "import easygems\n",
    "import easygems.remap as egr\n",
    "import easygems.healpix as egh\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "import zarr\n",
    "\n",
    "import shutil\n",
    "\n",
    "# notes\n",
    "# -----\n",
    "# - online sources suggest using healpy intead of healpix? \n",
    "#   + import healpy \n",
    "#   + Possibly healpix was added later and built on healpy.\n",
    "#   + HEALPix documentation: https://healpix.sourceforge.io/documentation.php\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Numpy {np.__version__}\")\n",
    "print(f\"Xarray {xr.__version__}\")\n",
    "print(f\"Dask {dask.__version__}\")\n",
    "\n",
    "print(f\"Healpix {hp.__version__}\")\n",
    "print(f\"Zarr {zarr.__version__}\")\n",
    "\n",
    "print(f\"EasyGems doesn't provide a version attribute.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # SET NECESSARY INPUT AND OUTPUT PATHS\n",
    "\n",
    "    # dataloc = Path(\"/glade/campaign/mmm/wmr/skamaroc/NSC_2023/3.75km_simulation_output_save\")\n",
    "    # dataloc = Path(\"/glade/derecho/scratch/brianpm/healpix\")\n",
    "    # datafil = dataloc / \"diag.3.75km.2020-10-21_07.30.00.nc\"\n",
    "\n",
    "    dataloc = Path(\"/glade/campaign/mmm/wmr/skamaroc/NSC_2023/3.75km_simulation_output_old_transport\")\n",
    "\n",
    "    datafils = sorted(dataloc.glob(\"DYAMOND_diag_3h.3.75km.*.nc\")) # note: 70GB per file\n",
    "\n",
    "    print(f\"Identified {len(datafils)} files to remap to healpix and save to zarr.\")\n",
    "\n",
    "    # mesh description (maybe)\n",
    "    meshloc = Path(\"/glade/campaign/mmm/wmr/skamaroc/NSC_2023\")\n",
    "    meshfil = meshloc / \"x1.41943042.static.nc\"\n",
    "\n",
    "    # output location\n",
    "    # oloc = Path(\"/glade/derecho/scratch/brianpm/healpix\")\n",
    "    oloc = Path(\"/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3\")\n",
    "\n",
    "    # Set parameters needed for generation weights:\n",
    "    zoom = order = 10\n",
    "\n",
    "    weights_file = oloc / f\"mpas_to_healpix_weights_order{zoom}.nc\"\n",
    "\n",
    "    vert_weights_file = oloc / f\"mpas_to_healpix_vertex_weights_order{zoom}.nc\"\n",
    "\n",
    "    out_prefix = \"FIX_DYAMOND_diag_3h\"\n",
    "\n",
    "    overwrite_weights = False\n",
    "\n",
    "    overwrite_zarr = False\n",
    "\n",
    "    # mpas_to_hp_zarr(ds_mpas_clean, ds_static, zoom, weights_file, vert_weights_file, oloc, out_prefix, clobber_wgts=False, clobber_zarr=True)\n",
    "\n",
    "    # cell-center\n",
    "    ds_static = xr.open_dataset(meshfil)\n",
    "    lon, lat = get_mpas_lonlat(ds_static, 'lonCell', 'latCell', degrees=True, negative=True, verbose=True)\n",
    "    vlon, vlat = get_mpas_lonlat(ds_static, 'lonVertex', 'latVertex', degrees=True, negative=True, verbose=True)\n",
    "\n",
    "    # generate or load weights\n",
    "    eweights = get_weights_to_healpix(lon, lat, zoom, weights_file, overwrite=overwrite_weights)\n",
    "    evweights = get_weights_to_healpix(vlon, vlat, zoom, vert_weights_file, overwrite=overwrite_weights) # not needed for DYAMOND_diag_3h files\n",
    "\n",
    "    for i, fil in enumerate(datafils[0:3]):\n",
    "        print(f\"Processing file {i+1}/{len(datafils)}: {fil.name}\")\n",
    "        data = pre_proc_mpas_file(fil, ds_static)\n",
    "        dsout = remap_mpas_to_hp(data, eweights, evweights, order)\n",
    "\n",
    "        # save highest resolution output\n",
    "        fn = oloc / f\"{out_prefix}_to_hp{order}.zarr\"\n",
    "        save_to_zarr(dsout, fn, clobber=overwrite_zarr)\n",
    "\n",
    "        # now coarsen and save zarr\n",
    "        mpas_hp_to_zarr(dsout, order, oloc, out_prefix, clobber=overwrite_zarr)\n",
    "\n",
    "\n",
    "\n",
    "def pre_proc_mpas_file(datafil, meshfil):\n",
    "    ds_mpas = xr.open_dataset(datafil, engine='netcdf4',  chunks={'Time': 'auto'})\n",
    "    if isinstance(meshfil, xr.Dataset):\n",
    "        ds_static = meshfil\n",
    "    elif isinstance(meshfil, Path):\n",
    "        ds_static = xr.open_dataset(meshfil)\n",
    "    else:\n",
    "        raise ValueError(\"meshfil needs to be a dataset or a path\")\n",
    "\n",
    "    # Clean and convert xtime strings\n",
    "    time_str = ds_mpas.xtime.astype(str).values.astype('U').ravel()\n",
    "    # Remove extra whitespace and handle empty strings\n",
    "    time_str = [x.strip() for x in time_str]\n",
    "    time_str = [x.replace(\"_\", \" \") for x in time_str]\n",
    "\n",
    "    # Convert to datetime\n",
    "    # change coordinate (and index) from \"Time\" to \"time\"\n",
    "    time_coord = pd.to_datetime(time_str)\n",
    "\n",
    "    ds_mpas_new = ds_mpas.assign_coords(time=('Time', time_coord))\n",
    "    \n",
    "    ds_mpas_new = ds_mpas_new.swap_dims({\"Time\":\"time\"})\n",
    "\n",
    "    # Find variables with dtype 'S64'\n",
    "    s64_vars = [var for var in ds_mpas_new.variables if ds_mpas_new[var].dtype == 'S64']\n",
    "    print(f\"Variables with S64 dtype: {s64_vars}\")\n",
    "\n",
    "    # Drop these variables from the dataset\n",
    "    ds_mpas_clean = ds_mpas_new.drop_vars(s64_vars)\n",
    "\n",
    "    return ds_mpas_clean\n",
    "\n",
    "# All the local functions that we need.\n",
    "\n",
    "def remove_directory(inpath):\n",
    "    \"\"\"\n",
    "    Removes a directory and its contents recursively.\n",
    "\n",
    "    Args:\n",
    "        path_str: Path to the directory as a string.\n",
    "    \"\"\"\n",
    "    if isinstance(inpath, str):\n",
    "        path = Path(inpath)\n",
    "    else:\n",
    "        path = inpath\n",
    "    if path.exists():\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Directory not found: {path}\")\n",
    "\n",
    "\n",
    "def get_mpas_lonlat(ds, lonname, latname, degrees=True, negative=True, verbose=False):\n",
    "    '''Get latitude and longitude from MPAS \"static\" file,\n",
    "       convert to degrees (default),\n",
    "       convert to [-180, 180] convention (default)\n",
    "\n",
    "    ds : xr.Dataset\n",
    "        data set that needs to have lat and lon values\n",
    "    latname : str\n",
    "        name of the latitude variable\n",
    "    lonname : str\n",
    "        name of the longitude variable\n",
    "    degrees : bool\n",
    "        if true, convert to degrees (ASSUMES RADIANS)\n",
    "    negative : bool\n",
    "        if true, convert to -180 format if needed\n",
    "        if false, convert to 360 format if needed\n",
    "        Assumes unit is degrees, and the conversion is based on minimum longitude value being < 0 or maximum > 180\n",
    "        Does not \"roll\" the coordinate (i.e. change the order of the longitudes)\n",
    "    verbose : bool\n",
    "        if true print stuff\n",
    "    '''\n",
    "    lonrad = ds[lonname]\n",
    "    latrad = ds[latname]\n",
    "    if verbose:\n",
    "        print(f\"Sizes: {lonrad.shape = }, {latrad.shape = } -- Compare with {ds['nCells'].shape}\")\n",
    "        print(f\"[initial] Lat min/max: {latrad.min().item()}, {latrad.max().item()}, Lon min/max: {lonrad.min().item()},{lonrad.max().item()}\")\n",
    "    \n",
    "    if degrees:\n",
    "        # lon and lat are in radians\n",
    "        lon = np.rad2deg(lonrad) \n",
    "        lat = np.rad2deg(latrad)\n",
    "    else:\n",
    "        lon = lonrad\n",
    "        lat = latrad\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[degrees] Lat min/max: {lat.min().item()}, {lat.max().item()}, Lon min/max: {lon.min().item()},{lon.max().item()}\")\n",
    "\n",
    "    if negative:\n",
    "        if lon.max().item() >= 180:\n",
    "            lon=(lon + 180) % 360 - 180  # [-180, 180)\n",
    "    else:\n",
    "        if lon.min().item() < 0:\n",
    "            lon += 180\n",
    "    if verbose:\n",
    "        print(f\"[final] Lat min/max: {lat.min().item()}, {lat.max().item()}, Lon min/max: {lon.min().item()},{lon.max().item()}\")\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "def get_weights_to_healpix(lon, lat, order, weights_file, overwrite=None):\n",
    "    # nside determines the resolution of the map, generally a power of 2\n",
    "    # zoom & order are just the exponent:\n",
    "    # nside = 2**(zoom)\n",
    "\n",
    "    # npix is just the number of \"pixels\" (grid points on HEALPix grid)\n",
    "    zoom = order\n",
    "    nside = hp.order2nside(order)\n",
    "    npix = hp.nside2npix(nside)\n",
    "\n",
    "    write = False\n",
    "    if weights_file.is_file():\n",
    "        if overwrite:\n",
    "            write = True\n",
    "            weights_file.unlink()\n",
    "            print(\"Overwrite existing file.\")\n",
    "    else:\n",
    "        write = True\n",
    "\n",
    "    latlon = True\n",
    "\n",
    "    print(f\"The number of pixels is {npix}, based on {nside} = 2**{zoom}. WRITE: {write}. LATLON: {latlon}\")\n",
    "\n",
    "    if write:\n",
    "        # gets the longitude and latitude of each\n",
    "        # latlon: If True, input angles are assumed to be longitude and latitude in degree, otherwise, they are co-latitude and longitude in radians.\n",
    "        hp_lon, hp_lat = hp.pix2ang(nside=nside, ipix=np.arange(npix), lonlat=latlon, nest=True)\n",
    "\n",
    "        # WE NEED TO SHIFT LONGITUDE TO [-180,180] CONVENTION\n",
    "        # Probably only if source does??\n",
    "        if latlon and np.any(hp_lon > 180):\n",
    "            hp_lon = (hp_lon + 180) % 360 - 180  # [-180, 180)\n",
    "            hp_lon += 360 / (4 * nside) / 4  # shift quarter-width  ##???????##\n",
    "            # source lon shift already applied using get_mpas_lonlat\n",
    "        else:\n",
    "            print(f\"Will not modify hp_lon. Min/Max: {hp_lon.min().item()}, {hp_lon.max().item()} Size: {hp_lon.shape}\")\n",
    "\n",
    "        # easygems weight generation\n",
    "        # If latlon=True above, then we probably want source in degrees\n",
    "        eweights = egr.compute_weights_delaunay((lon, lat),(hp_lon, hp_lat))\n",
    "\n",
    "        # save the calculated weights for future use    \n",
    "        eweights.to_netcdf(weights_file)\n",
    "        print(f\"Weights file written: {weights_file.name}\")\n",
    "        return eweights\n",
    "    else: \n",
    "        return xr.open_dataset(weights_file) \n",
    "\n",
    "    # NOTE: write=True takes a while: ~9min\n",
    "\n",
    "\n",
    "\n",
    "def apply_weights_hp(ds, weights, order, mpas_v_c=None):\n",
    "    \"\"\"remap to healpix using easygems generated weights\n",
    "    \n",
    "    ds and weights should be consistent\n",
    "    mpas_v_c determines if using \"nCell\" or \"nVertices\" variables\n",
    "    \"\"\"\n",
    "    assert (mpas_v_c in [\"center\", \"vertex\"]), f\"mpas_v_c must be center or vertex, got {mpas_v_c}\"\n",
    "    # repeat:\n",
    "    zoom = order\n",
    "    nside = hp.order2nside(order)\n",
    "    npix = hp.nside2npix(nside)\n",
    "\n",
    "    vertices_vars = []\n",
    "    center_vars = []\n",
    "    vars_to_drop = None\n",
    "    for v in ds:\n",
    "        if 'nVertices' in ds[v].dims:\n",
    "            vertices_vars.append(v)\n",
    "        elif 'nCells' in ds[v].dims:\n",
    "            center_vars.append(v)\n",
    "    if mpas_v_c == \"center\":\n",
    "        vars_to_drop = vertices_vars\n",
    "        core_dims_list = [\"nCells\"]\n",
    "    elif mpas_v_c == \"vertex\":\n",
    "        vars_to_drop = center_vars\n",
    "        core_dims_list = [\"nVertices\"]\n",
    "    if vars_to_drop:\n",
    "        ds_filter = ds.drop_vars(vars_to_drop)\n",
    "    else:\n",
    "        ds_filter = ds\n",
    "    \n",
    "    mpas_remap = xr.apply_ufunc(\n",
    "        egr.apply_weights,\n",
    "        ds_filter,\n",
    "        kwargs=weights,\n",
    "        keep_attrs=True,\n",
    "        input_core_dims=[core_dims_list],\n",
    "        output_core_dims=[[\"cell\"]],\n",
    "        on_missing_core_dim='copy',\n",
    "        output_dtypes=[\"f4\"],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs={\n",
    "            \"output_sizes\": {\"cell\": npix},\n",
    "        },\n",
    "    )\n",
    "    return mpas_remap\n",
    "\n",
    "\n",
    "def remap_mpas_to_hp(ds, cell_weights, vertex_weights, zoom):\n",
    "    c_vars = apply_weights_hp(ds, cell_weights, zoom, mpas_v_c=\"center\")\n",
    "    v_vars = apply_weights_hp(ds, vertex_weights, zoom, mpas_v_c=\"vertex\")\n",
    "    mrg = xr.merge([c_vars, v_vars])\n",
    "\n",
    "    # Add the CRS\n",
    "    mrg[\"crs\"] = xr.DataArray(\n",
    "        name=\"crs\",\n",
    "        data=0,\n",
    "        attrs={\n",
    "            \"grid_mapping_name\": \"healpix\",\n",
    "            \"healpix_nside\": 2**zoom,\n",
    "            \"healpix_order\": \"nest\",\n",
    "        },\n",
    "    )\n",
    "    return mrg\n",
    "\n",
    "# Write to ZARR\n",
    "\n",
    "def get_dtype(da):\n",
    "    if np.issubdtype(da.dtype, np.floating):\n",
    "        return \"float32\"\n",
    "    else:\n",
    "        return da.dtype\n",
    "\n",
    "def get_encoding(dataset):\n",
    "    return {\n",
    "        var: {\n",
    "            # \"compressor\": get_compressor(),\n",
    "            \"dtype\": get_dtype(dataset[var]),\n",
    "            # \"chunks\": get_chunks(dataset[var].dims),\n",
    "        }\n",
    "        for var in dataset.variables\n",
    "        if var not in dataset.dims\n",
    "    }\n",
    "\n",
    "\n",
    "def save_to_zarr(ds, fn, clobber=None):\n",
    "    if fn.exists():\n",
    "        if clobber:\n",
    "            print(f\"{fn} exists... remove\")\n",
    "            do_save = True\n",
    "            remove_directory(fn)\n",
    "        else:\n",
    "            print(f\"{fn} exists... coarsen but do not save\")\n",
    "            do_save = False\n",
    "    else:\n",
    "        do_save = True\n",
    "    if do_save:\n",
    "        store = zarr.storage.LocalStore(fn)\n",
    "        if fn.exists():\n",
    "            # If the store exists, append to it\n",
    "            ds.chunk({\"time\": -1, \"cell\": -1}).to_zarr(store, encoding=get_encoding(ds), append_dim='time', consolidated=False)\n",
    "        else:\n",
    "            # For the first write, don't use append_dim\n",
    "            ds.chunk({\"time\": -1, \"cell\": -1}).to_zarr(store, encoding=get_encoding(ds), consolidated=False)\n",
    "        print(f'Saved: {str(fn)}')\n",
    "    else:\n",
    "        print('Determined not to save to zarr.')\n",
    "\n",
    "\n",
    "def mpas_hp_to_zarr(ds, zoom, outloc, zarr_name_prefix, clobber=None):\n",
    "    \"\"\"Save to zarr at zoom and lower resolutions\n",
    "    \n",
    "    notes\n",
    "    -----\n",
    "    the iteration is from zoom-1 down to zero. Have to use\n",
    "    zoom-1 because the computation is to coarsen from the \"current\"\n",
    "    healpix level down to the next one.\n",
    "    \"\"\"\n",
    "    dn=ds.copy()\n",
    "    for x in range(zoom-1,0,-1):\n",
    "        fn = outloc / f\"{zarr_name_prefix}_to_hp{x}.zarr\"\n",
    "\n",
    "        # coarsen by one level\n",
    "        dx = dn.coarsen(cell=4).mean()\n",
    "        save_to_zarr(dx, fn, clobber=clobber)\n",
    "        # iterate\n",
    "        dn = dx.copy()\n",
    "    print(\"[mpas_hp_to_zarr] complete.\")\n",
    "\n",
    "\n",
    "def mpas_to_hp_zarr(data, grid_data, order, c_weights, v_weights, out_dir, zarr_prefix, clobber_wgts=None, clobber_zarr=None):\n",
    "    # cell-center\n",
    "    lon, lat = get_mpas_lonlat(grid_data, 'lonCell', 'latCell', degrees=True, negative=True, verbose=True)\n",
    "\n",
    "    # generate or load weights\n",
    "    eweights = get_weights_to_healpix(lon, lat, order, c_weights, overwrite=clobber_wgts)\n",
    "\n",
    "    # MPAS files have variables at cell centers and vertices,\n",
    "    # to remap them weights for each are needed:\n",
    "\n",
    "    vlon, vlat = get_mpas_lonlat(grid_data, 'lonVertex', 'latVertex', degrees=True, negative=True, verbose=True)\n",
    "\n",
    "    evweights = get_weights_to_healpix(vlon, vlat, order, v_weights, overwrite=clobber_wgts)\n",
    "\n",
    "    dsout = remap_mpas_to_hp(data, eweights, evweights, order)\n",
    "\n",
    "    # save highest resolution output\n",
    "    fn = out_dir / f\"{zarr_prefix}_to_hp{order}.zarr\"\n",
    "    save_to_zarr(dsout, fn, clobber=clobber_zarr)\n",
    "\n",
    "    # now coarsen and save zarr\n",
    "    mpas_hp_to_zarr(dsout, order, out_dir, zarr_prefix, clobber=clobber_zarr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 361 files to remap to healpix and save to zarr.\n",
      "Sizes: lonrad.shape = (41943042,), latrad.shape = (41943042,) -- Compare with (41943042,)\n",
      "[initial] Lat min/max: -1.5707963705062866, 1.5707963705062866, Lon min/max: 0.0,6.2831854820251465\n",
      "[degrees] Lat min/max: -90.0, 90.0, Lon min/max: 0.0,360.0\n",
      "[final] Lat min/max: -90.0, 90.0, Lon min/max: -180.0,179.98284912109375\n",
      "Sizes: lonrad.shape = (83886080,), latrad.shape = (83886080,) -- Compare with (41943042,)\n",
      "[initial] Lat min/max: -1.5704516172409058, 1.5704516172409058, Lon min/max: 0.0,6.2831854820251465\n",
      "[degrees] Lat min/max: -89.9802474975586, 89.9802474975586, Lon min/max: 0.0,360.0\n",
      "[final] Lat min/max: -89.9802474975586, 89.9802474975586, Lon min/max: -180.0,179.98941040039062\n",
      "The number of pixels is 12582912, based on 1024 = 2**10. WRITE: False. LATLON: True\n",
      "The number of pixels is 12582912, based on 1024 = 2**10. WRITE: False. LATLON: True\n",
      "Processing file 1/361: DYAMOND_diag_3h.3.75km.2020-01-20_00.00.00.nc\n",
      "Variables with S64 dtype: ['initial_time', 'xtime']\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp10.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp9.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp8.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp7.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp6.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp5.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp4.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp3.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp2.zarr\n",
      "Saved: /glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp1.zarr\n",
      "[mpas_hp_to_zarr] complete.\n",
      "Processing file 2/361: DYAMOND_diag_3h.3.75km.2020-01-20_03.00.00.nc\n",
      "Variables with S64 dtype: ['initial_time', 'xtime']\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp10.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp9.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp8.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp7.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp6.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp5.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp4.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp3.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp2.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp1.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "[mpas_hp_to_zarr] complete.\n",
      "Processing file 3/361: DYAMOND_diag_3h.3.75km.2020-01-20_06.00.00.nc\n",
      "Variables with S64 dtype: ['initial_time', 'xtime']\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp10.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp9.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp8.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp7.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp6.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp5.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp4.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp3.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp2.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "/glade/derecho/scratch/digital-earths-hackathon/mpas_DYAMOND3/FIX_DYAMOND_diag_3h_to_hp1.zarr exists... coarsen but do not save\n",
      "Determined not to save to zarr.\n",
      "[mpas_hp_to_zarr] complete.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
